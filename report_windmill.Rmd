---
title: "Juntos Project Initial Report"
subtitle: "Educators' Baseline Assessment Cleaning and Recommendations"
author: "Alejandra Garcia Isaza"
date: "March 2021"
output: 
  pagedreport::paged_windmill:
    front_img: "../juntos/images/front.PNG"
    logo: "../juntos/images/ceqp_logo.svg"
    img_to_dark: TRUE
    logo_to_white: FALSE
knit: pagedown::chrome_print
main-color: "#65955E"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)

library(rio)
library(here)
library(tidyverse)
library(haven)
library(janitor)
library(knitr)
library(surveytoolbox)
library(sjPlot)
library(kableExtra)
library(psych)

```

# Juntos Project Description {-}

## Study and intervention details {-}

The *Juntos* Project was a three-year study led by the University of Oregon’s Center for Equity Promotion [CEQP.](https://ceqp.uoregon.edu/). The project developed a culturally specific family–school partnership intervention, *Conexiones: Families and Schools United for Equity* (hereafter referred to as *Conexiones*), designed to enhance Latino parents’ and educators’ capacities to effectively support Latino student success.

The *Conexiones* curricula was built on Latino cultural assets, addressed common challenges confronting immigrant students and families in terms of school success, and utilized effective strategies for increasing educators’ awareness of Latino cultures and the barriers that exist for Latino immigrant students and families in schools. It also focused on building effective family-school communication and partnerships with the aim of improving Latino students’ academic success. 

The six participating schools belonged to three different school districts in the state of Oregon and were randomly assigned to either a control group or a intervention group that received the *Conexiones* intervention program. Study participants completed assessments at three different time points (baseline, immediately post-intervention, and 12-month post-intervention). The complete dataset in the project is made of three waves of data with separate assessments for each participant type (parents, students, and educators). 

```{r echo=FALSE, out.width="50%", fig.align="center"}

knitr::include_graphics("../juntos/images/ceqp_logo.PNG")
```

\pagebreak

## Report details {-}

This report focuses only on the educators' baseline assessment and is intended to describe the data cleaning process with the aim of guiding CEQP's researchers and data analysts in the procedures performed to the dataset. A secondary aim is to help CEQP staff with data management responsibilities to replicate these procedures in subsequent waves of data and future projects. 

The report will also include a brief description of the sociodemographic characteristics of the study participants, the scale creation process, the average scores of participants’ responses in regards to major study constructs, and recommendations for data management and cleaning. 

In the appendix section, data analysts interested in using this dataset will find a codebook with all the items, variable names, and response options. 


```{r echo=FALSE, out.width="75%", fig.align="center"}

knitr::include_graphics("../juntos/images/CEQP_final_panel_3.jpg")
```


```{r echo=FALSE, out.width="75%", fig.align="center"}

knitr::include_graphics("../juntos/images/CEQP_final_panel_6.jpg")
```

# Data Cleaning procedures {-}

The following section describes the data cleaning procedures I performed in the baseline assessment of the educators' dataset.

I performed data cleaning procedures using the [R](https://www.r-project.org/) and [R Studio](https://rstudio.com/) softwares, but had in mind that end users of the cleaned datasets will likely be SPSS users, thus, I exported the cleaned dataset to a *.sav* file.

## The dataset {-}

```{r include=FALSE}

w1_raw_elt <- read_sav(here("nopublish", "ELT W1 ERC 11.11.2020.sav"))
```

The raw dataset had `r nrow(w1_raw_elt)` observations and `r ncol(w1_raw_elt)` variables of which `r ncol(w1_raw_elt %>% select(1:17))` were metadata variables created by Qualtrics, the software used to develop the assessment surveys. Of the `r nrow(w1_raw_elt)` observations, one case, participant with `id` `r w1_raw_elt$PJ__[10]` had incomplete data. I called the raw dataset downloaded directly from Qualtrics as `w1_raw_elt` which stands for wave one of the raw data from the equity leadership team (i.e. elt).

## Initial cleaning {-}

In the following code, I created a new dataframe `elt_w1_clean` where I selected out all but one of the metadata variables, `response_id`. This variable is an unique identifier assigned by Qualtrics that resulted handy in dealing with duplicated ids. 

Other simple data cleaning procedures are noted in the comments marked with a # sign. I used the [clean_names](https://www.rdocumentation.org/packages/janitor/versions/1.2.0/topics/clean_names), [select](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/select), [rename](https://www.rdocumentation.org/packages/plyr/versions/1.8.6/topics/rename), and [arrange](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/arrange) functions. 

```{r}

elt_w1_clean <- w1_raw_elt %>% 
  clean_names() %>% # function that formats variables' names
  select(-1:-8, -10:-17, -202) %>% # selecting out columns with metadata
  rename(c("id" = "pj")) %>% # renaming id variable.
  arrange(id) # ordering participants ids in descending order
```

## Dealing with duplicated ids {-}

When evaluating if the dataframe had duplicated ids, I found that `id` 257 was duplicated and there was no `id` 254. 

In the table below, I am just showing a few variables and participants from `school` 2. 


```{r echo=FALSE}

d <- elt_w1_clean %>%
  select(1:3, 7:9) %>%
  filter(id > 250 & id < 350)
```

<p>&nbsp;</p>

```{r echo=FALSE}

d %>%
  kbl() %>%
  kable_material(c("striped", "hover", font_size = 7)) %>%
  row_spec(6, color = "black", background = "#F3E35A")
```

After checking with CEQP's research assistant, I corroborated that one of the duplicated cases of `id` 257 in fact was `id` 254. I fixed this mistake with the code below using the `response_id` variable and the  [mutate](https://dplyr.tidyverse.org/reference/mutate.html) and [case_when](https://dplyr.tidyverse.org/reference/case_when.html) functions. 

The combination of these two functions is creating a new variable (that I am naming the same as it was, `id`) to follow the condition that if the variable `response_id` has the "R_6EELe7Uuwi9W7zX" value, the `id` value should be recoded as "254".  

```{r}

elt_w1_clean <- elt_w1_clean %>%
  mutate(id = case_when(response_id == "R_6EELe7Uuwi9W7zX" ~ "254",
                        TRUE ~ as.character(id))) %>%
  arrange(id)
```

## Dealing with survey coding errors {-}

The id protocol followed in CEQP projects is very straightforward. They use three digits for each individual participant id and use the first of these three digits to indicate the school id. In this system, ids in the 100’s would belong to school 1, ids in the 200’s to school 2, and so on.

By visual inspection I dentified that the first digit of the indvidual ids in the `id` variable did not correspond to the ids in the school id variable `school` for schools 3, 4, 5, and 6. In the table below, I selected four variables and only the first row of data of each of the six schools to ilustrate this point. 

```{r echo=FALSE}

d2 <- elt_w1_clean %>%
  select(2, 3, 7:9) %>%
  filter(id == 150 | id == 250 |id == 350 |id == 450 |id == 550 |id == 650)
```

```{r echo=FALSE}

d2 %>%
  kbl() %>%
  kable_material(c("striped", "hover", font_size = 7)) %>%
  row_spec(3:6, color = "black", background = "#F3E35A")
```

As can be seen in the table above, ids in the 300's are coded to belong to `school` 4 and ids in the 400's are coded to belong to `school` 3. I am calling this flip-flopped school ids. Schools 5 and 6 were also flip-flopped. 

At first, I thought that this could be due to an error in the data exporting process and it seemed like an easy enough fix to make. I thought I just needed to recode the names of the levels of the `school` variable. Later I found that this fix did not solve the issue. It took me a couple of months to identify that the error was coded in the Qualtrics survey.

\pagebreak

The images below are screenshots of the same raw data SPSS file downloaded directly from Qualtrics. In figure 1, it can be seen that when the *value labels* button is "on" (i.e. showing value labels and not values), it appears as if there was no flip-flop because the names of the schools coincided with the numbers that were assigned to them. Indeed, school "K" was school 3 and its participants were identified with ids in the 300's and school "A" was school 4 and its participants were identified with ids in the 400's, and so on.

```{r echo=FALSE, out.width="75%", fig.cap="Value labels button on.", fig.align="center"}

knitr::include_graphics("../juntos/images/val_lab_on.PNG")
```

\pagebreak

This changed when the *value labels* button was "off". In the image below, the flip-flopped school ids is evident again: 

```{r echo=FALSE, out.width="75%", fig.cap="Value labels button off", fig.align="center"}

knitr::include_graphics("../juntos/images/val_lab_off.PNG")
```

This survey coding error meant that the `school` variable's value labels properly corresponded to the participants' ids, but the variable's values did not. Instead of recoding the values, I decided to create a new variable called `school_id` and delete the flawed original variable `school`. 

In the code below, I created a new dataframe `elt_w1_clean_2` where I used the first digit of the individual participant id variable `id` as the reference for the new `school_id` variable, following CEQP'S id protocol. I also created a new variable called `condition` to indicate which schools were randomly assigned to the control group (coded as 1) or to the intervention group (coded as 2). 

I coded schools identified with a `school_id` odd number (1, 3, and 5) as the control schools and the schools identified with an even number (2, 4, and 6) as the intervention schools, as directed by CEQP's research assistant. Finally, I also created a `wave` variable to indicate the wave of the data. Note that I am creating all of these new variables with the [mutate](https://dplyr.tidyverse.org/reference/mutate.html) function.

\pagebreak

```{r}

elt_w1_clean_2 <- elt_w1_clean %>%
  mutate(school_id = str_sub(id, 1, 1), # new school id variable
         condition = case_when(
           school_id == "1" | school_id == "3" | school_id == "5" ~ "1",
           school_id == "2" | school_id == "4" | school_id == "6" ~ "2")) %>% # new condition variable
  select(school_id, condition, everything()) %>%
  add_column(wave = 1, .before = 9) %>% # new wave variable
  select(- school) # selecting out (i.e. deleting) school variable
```

The `condition` and `school_id` variables I created in the previous code were string variables. In the code below I created a new dataframe `elt_w1_clean_3` where I coerced these variables to be numeric so they can be used in quantitative analyses using the [as.numeric](https://www.rdocumentation.org/packages/h2o/versions/3.32.0.1/topics/as.numeric) function. I also added value labels with the [set_vall](https://nicedoc.io/martinctc/surveytoolbox) function so that SPSS users can use the *value labels* button. 

In the code below I also fixed a response option coding error I identified in the variable `q68`. Throughout most of the survey, response options were coded as "Strongly Disagree" = 1, "Disagree" = 2, "Agree" = 3, "Strongly Agree" = 4, "No response" = 99; however, in variable `q68` the response option "No response" was coded as "5". 

I fixed this using the [ifelse](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse) function, specifying that if this variable had a response of 5, it should be changed to 99. Finally, I set the variable and value labels with the [set_varl](https://nicedoc.io/martinctc/surveytoolbox) and [set_vall](https://nicedoc.io/martinctc/surveytoolbox) functions, respectively, because sometimes procedures performed with ceratin functions strips out these labels. 


```{r}

elt_w1_clean_3 <- elt_w1_clean_2 %>%
  mutate(condition = as.numeric(condition),
         condition = set_vall(condition, c("control" = 1, "intervention" = 2)),
         school_id = as.numeric(school_id),
         school_id = set_vall(school_id, c("cascade" = 1, "prairie_mountain" = 2, "kelly" = 3, "ata" = 4, "briggs" = 5, "agnes_stewart" = 6)),
         q68 = ifelse(q68 == 5, 99, q68),
         q68 = set_varl(q68, "When I communicate with Latino families, I keep in mind that many Latino parents may not understand how to navigate the educational system in this
country."),
         q68 = set_vall(q68, c("Strongly Disagree" = 1, "Disagree" = 2, "Agree" = 3, "Strongly Agree" = 4, "No response" = 99)))
```

## Dealing with split out responses {-}

In this dataset, several multiple choice variables that were originally meant to have a single answer, were spread out as if they allowed to have multiple answers. I believe this was because in the Qualtrics survey development process, the option for *Multiple answer* was selected, instead of *Single answer*. 

```{r echo=FALSE, out.width="75%", fig.cap="Qualtrics survey development", fig.align="center"}

knitr::include_graphics("../juntos/images/multi_answ.PNG")
```


When this happens, participants could select mutually exclusive options, like this:

```{r echo=FALSE, out.width="50%", fig.cap="Qualtrics survey preview", fig.align="center"}

knitr::include_graphics("../juntos/images/multi_answ_look.PNG")
```

\pagebreak

When *Multiple answer* is selected, Qualtrics splits these multi-value fields into columns, assigning a value of 1 if a response option is chosen and a value of 0 if a response option is not chosen. In the following code, I collapsed the Spanish variable that was split out so it could be easily used in analyses. To avoid overwhelming the reader, I am omitting the code I used to collapse other language variables. I used the same procedure in all of these variables. 

In the code below, the function [pivot_longer](https://tidyr.tidyverse.org/articles/pivot.html) makes the dataframe "long" as it increases the number of rows and decreases the number of columns. This function gathers variables' names under the new variable `item_2` and gathers the values of these variables under the new variable `spanish_comfort`. Then, I chose only the options that had a value of 1, indicating when a participant chose that response option. 

Finally, I recoded the response options to follow this scheme: "Not at all comfortable" = 1, "Somewhat comfortable" = 2, "Comfortable" = 3, "Very comfortable" = 4, "No response" = 99. 

```{r include=FALSE}
# NOT including this code chunk in report. Just adding it to deal with a pandoc error. 

# collapsing english variables
eng <- elt_w1_clean_3 %>% 
  select(id, starts_with("q132_1")) %>% 
    pivot_longer(
      cols = starts_with("q132_1"),
      names_to = "item_1",
      values_to = "english_comfort",
      values_drop_na = TRUE) %>% 
   filter(english_comfort == 1) %>%
  mutate(english_comfort = case_when(item_1 == "q132_1_1" ~ "1",
                                     item_1 == "q132_1_2" ~ "2",
                                     item_1 == "q132_1_3" ~ "3",
                                     item_1 == "q132_1_4" ~ "4",
                                     item_1 == "q132_1_99" ~ "99",
                        TRUE ~ as.character(english_comfort)))  %>%
  select(-item_1)

# collapsing spanish variables
spa <- elt_w1_clean_3 %>%
  select(id, starts_with("q132_2")) %>% 
    pivot_longer(
      cols = starts_with("q132_2"),
      names_to = "item_2",
      values_to = "spanish_comfort",
      values_drop_na = TRUE) %>% 
   filter(spanish_comfort == 1) %>%
  mutate(spanish_comfort = case_when(item_2 == "q132_2_1" ~ "1",
                                     item_2 == "q132_2_2" ~ "2",
                                     item_2 == "q132_2_3" ~ "3",
                                     item_2 == "q132_2_4" ~ "4",
                                     item_2 == "q132_2_99" ~ "99",
                        TRUE ~ as.character(spanish_comfort))) %>%
  select(-item_2) 

# id 454 is duplicated. In w1_raw it shows that this participant chose options 1 and 2.

# fixing duplicate
spa_2 <- spa %>% 
  distinct(id, .keep_all = TRUE) # Option 1, "not at all comfortable" was chosen because the duplicate was the second option.  

# joining english and spanish variables
langs <- left_join(eng, spa_2) 

# collapsing other language 1 variables
other_1 <- elt_w1_clean_3 %>% 
  select(id, starts_with("q132_3"), -q132_3_text) %>% 
    pivot_longer(
      cols = starts_with("q132_3"),
      names_to = "item_3",
      values_to = "other1_lang_comfort",
      values_drop_na = TRUE) %>% 
   filter(other1_lang_comfort == 1) %>%
  mutate(other1_lang_comfort = case_when(item_3 == "q132_3_1" ~ "1",
                                    item_3 == "q132_3_2" ~ "2",
                                    item_3 == "q132_3_3" ~ "3",
                                    item_3 == "q132_3_4" ~ "4",
                                    item_3 == "q132_3_99" ~ "99",
                        TRUE ~ as.character(other1_lang_comfort)))  %>%
  select(-item_3)

# collapsing other language 2 variables
other_2 <- elt_w1_clean_3 %>% 
  select(id, starts_with("q132_4"), -q132_4_text) %>% 
    pivot_longer(
      cols = starts_with("q132_4"),
      names_to = "item_4",
      values_to = "other2_lang_comfort",
      values_drop_na = TRUE) %>% 
   filter(other2_lang_comfort == 1) %>%
  mutate(other2_lang_comfort = case_when(item_4 == "q132_4_1" ~ "1",
                                    item_4 == "q132_4_2" ~ "2",
                                    item_4 == "q132_4_3" ~ "3",
                                    item_4 == "q132_4_4" ~ "4",
                                    item_4 == "q132_4_99" ~ "99",
                        TRUE ~ as.character(other2_lang_comfort)))  %>%
  select(-item_4)


# joining other 1 and 2 language variables
others <- left_join(other_1, other_2) 


# joining dataframes with all language variables
lang_vars <- left_join(langs, others)


# renaming variables with text input in master dataset (n = 43)
lang_inputs <- elt_w1_clean_3 %>%
  select(id, q132_3_text, q132_4_text) %>%
  rename(c("other1_lang" = "q132_3_text"), c("other2_lang" = "q132_4_text"))

# joining renamed master dataset with all language variables now collapsed to "recover" id 153 data
all_lang_vars <- left_join(lang_inputs, lang_vars)

# recordering all language vars
all_lang_vars <- all_lang_vars %>%
  select(id, english_comfort, spanish_comfort, other1_lang, other1_lang_comfort, other2_lang, everything())

# creating new dataset that joins master dataset with reordered and collapsed lang vars
elt_w1_clean_4 <- left_join(elt_w1_clean_3, all_lang_vars) %>%
  select(-starts_with("q132_")) # deleting all previous language variables that were included in new variables  

# adding value labels on language vars
elt_w1_clean_4 <- elt_w1_clean_4 %>%
  mutate(english_comfort = as.numeric(english_comfort),
         english_comfort = set_vall(english_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         spanish_comfort = as.numeric(spanish_comfort),
         spanish_comfort = set_vall(spanish_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         other1_lang_comfort = as.numeric(other1_lang_comfort),
         other1_lang_comfort = set_vall(other1_lang_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         other2_lang_comfort = as.numeric(other2_lang_comfort),
         other2_lang_comfort = set_vall(other2_lang_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         )
```


```{r eval=FALSE}

# collapsing spanish variables
spa <- elt_w1_clean_3 %>%
  select(id, starts_with("q132_2")) %>% # creating a dataframe with only the id and Spanish variables
    pivot_longer(
      cols = starts_with("q132_2"), 
      names_to = "item_2",
      values_to = "spanish_comfort",
      values_drop_na = TRUE) %>% 
   filter(spanish_comfort == 1) %>%
  mutate(spanish_comfort = case_when(item_2 == "q132_2_1" ~ "1",
                                     item_2 == "q132_2_2" ~ "2",
                                     item_2 == "q132_2_3" ~ "3",
                                     item_2 == "q132_2_4" ~ "4",
                                     item_2 == "q132_2_99" ~ "99",
                        TRUE ~ as.character(spanish_comfort))) %>%
  select(-item_2) # selecting out variable with repetitive information
```

When all the language variables were collapsed I checked if there were duplicated cases and I found that participant identified with `id` 454 chose response option "1" and response option "2". 

\pagebreak

```{r echo=FALSE}

d3 <- spa %>%
  filter(id > 450 & id < 550)
```


```{r echo=FALSE}

d3 %>%
  kbl() %>%
  kable_material(c("striped", "hover", font_size = 7)) %>%
  row_spec(4:5, color = "black", background = "#F3E35A")
```

Because I can only assume that this was an entry error given that the choices are, in theory, mutually exclusive: "Not at all comfortable" = 1, vs. "Somewhat comfortable" = 2, I used the [distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/distinct) function to retain only unique values. 

For this case, option 1 = "Not at all comfortable" was retained as the function "assumes" the second option is the duplicative. 

```{r eval=FALSE}

spa_2 <- spa %>% 
  distinct(id, .keep_all = TRUE)
```

```{r echo=FALSE}

spa_2 %>%
  filter(id > 450 & id < 550) %>%
  kbl() %>%
  kable_material(c("striped", "hover", font_size = 7)) %>%
  row_spec(4, color = "black", background = "#F3E35A")
```


The last step in this process was joining the dataframe I created with all the language variables I collapsed (`all_lang_vars`) and the `elt_w1_clean_3` dataframe that had  the remaining variables. I used the [left_join](https://www.rdocumentation.org/packages/tidytable/versions/0.5.9/topics/left_join.) function to do this. 

In this new dataframe `elt_w1_clean_4` I also coerced the language variables to become numeric so they could be used in quantitative analyses and added the value labels so that SPSS users can use the *value labels* button. I used the code below to do this. 

```{r eval=FALSE, message=FALSE}

elt_w1_clean_4 <- left_join(elt_w1_clean_3, all_lang_vars) %>%
  select(-starts_with("q132_")) # selecting out language variables included now in new language Variables  

elt_w1_clean_4 <- elt_w1_clean_4 %>%
  mutate(english_comfort = as.numeric(english_comfort),
         english_comfort = set_vall(english_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         spanish_comfort = as.numeric(spanish_comfort),
         spanish_comfort = set_vall(spanish_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         other1_lang_comfort = as.numeric(other1_lang_comfort),
         other1_lang_comfort = set_vall(other1_lang_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)),
         other2_lang_comfort = as.numeric(other2_lang_comfort),
         other2_lang_comfort = set_vall(other2_lang_comfort, c("not at all comfortable" = 1, "somewhat comfortable" = 2, "comfortable" = 3, "very comfortable" = 4, "no response" = 99)))
```

As shown, the following variables were the result of the collapsing process described above: `english_comfort`, `spanish_comfort`, `other1_lang_comfort`, and `other2_lang_comfort`. 

## Renaming demographic variables {-}

In the code below, I created a new dataframe `elt_w1_clean_5` where I used the [rename](https://www.rdocumentation.org/packages/reshape/versions/0.8.8/topics/rename) function to rename some of the demographic variables that I used to describe participants's characteristics in the next section of this report. This function uses a "new name" = "old name" pattern. Very straightforward! 

At the end I selected out a few variables that did not have meaningful information. For instance, variable `q127` was a response/no response question that only indicated if participants chose to answer it. The meaningul information was contained in variable `q127_1_text` that was renamed as `age`, which I also coerced to become a numeric variable.  

```{r}

elt_w1_clean_5 <- elt_w1_clean_4 %>%
  rename(c("age" = "q127_1_text"), 
         c("birth_country" = "q128"),
         c("another_birth_country_text" = "q128_2_text"),
         c("age_first_moved_us" = "q129_1_text"),
         c("white" = "q130_1"),
         c("hispanic_latino_spanish" = "q130_2"),
         c("black_african_american" = "q130_3"),
         c("asian" = "q130_4"),
         c("american_indian_alaska_native" = "q130_5"),
         c("indigenous_americas" = "q130_6"),
         c("middle_eastern_north_african" = "q130_7"),
         c("native_hawaiian_pacific_islander" = "q130_8"),
         c("race_ethnicity_other" = "q130_9"),
         c("race_ethnicity_no_response" = "q130_99"),
         c("indigenous_americas_text" = "q130_6_text"),
         c("race_ethnicity_other_text" = "q130_9_text"),
         c("gender_id" = "q131"),
         c("years_in_position" = "q133"),
         c("years_in_school" = "q134"),
         c("equity_leadership" = "q135_1"),
         c("cultural_responsiveness" = "q135_2"),
         c("restorative_practices" = "q135_3"),
         c("diversity" = "q135_4"),
         c("ell" = "q135_5"),
         c("cont_ed_other" = "q135_6"),
         c("cont_ed_na" = "q135_88"),
         c("cont_ed_no_response" = "q135_99"),
         c("cont_ed_other_text" = "q135_6_text")) %>%
  mutate(age = as.numeric(age)) %>% # making variable numeric for QUAN analyses
  select(-q127, -q129, -q131_3_text) # selecting out because they did not have meaningful info
```


# Participant descriptives  {-}

```{r include=FALSE}
# recoding missing values as N/A with function

# vector with missing values in dataset
missing_vals <- c(88, 99, -99)

# function that returns true if values in vector are equal to missing_vals. The function takes a vector x, and specified values of missing data
recode_missing <- function(x, missing_vals = c(88, 99, -99)) {
  test <- x %in% missing_vals
  ifelse(test, NA, x)
}

# function that recodes missing values to NA. The function takes a dataframe with variables with missing data, and specified values of missing data
recode_missing_df <- function(df, missing_vals = c(88, 99, -99)) {
  modify(df, ~recode_missing(.x, missing_vals)) # here uses the function created above
}
```

In the following section, I used descriptive statistics to summarize participants' characteristics. In this analysis, I treated responses such as "99 = not applicable" or "88 = no response" as missing values. Other category of missing values were responses coded by Qualtrics as "-99 = seen but unanswered", when participants were not forced to respond. 

In the code below I created a new dataframe called `elt_w1_clean_6` where I applied a function I created `recode_missing_df` (code of the function not shown) to recode the 88, 99, and -99 values as `NA`, the way R codes missing values. 

```{r}

elt_w1_clean_6 <- recode_missing_df(elt_w1_clean_5) 
```

The downside of this function is that it strips out the variable and value labels, but having `NA` instead of the 88, 99, and -99 values is required to perform analyses in R. 

For this reason, the final dataset that will be exported to an SPSS file will still have the 88, 99, and -99 values. In the final cleaned dataset, SPSS users should manually code these values before performing analyses as *system myssing*. 

\pagebreak

## Educator's characteristics {-}

Educators in this first wave of data (*n* = `r nrow(elt_w1_clean_6)`) had a mean age of `r round(mean(elt_w1_clean_6[["age"]], na.rm = TRUE), 2)` years, with an age range between `r min(elt_w1_clean_6[["age"]], na.rm = TRUE)` and `r max(elt_w1_clean_6[["age"]], na.rm = TRUE)` years of age (see figure 5). The majority of educators were identified as female (74%).  

```{r echo=FALSE, warning=FALSE, fig.height= 4, fig.width=6, fig.cap="Educator's Age by gender", fig.align="center"}

viz_1 <- elt_w1_clean_6 %>%
  filter(response_id != "R_3dFdFPhdRJorWhT") %>%
  mutate(gender_id = as.factor(gender_id),
         gender_id = fct_recode(gender_id,
                           "Male" = "1",
                            "Female" = "2"))
  
ggplot(viz_1, aes(age)) +
  geom_histogram(fill = "#D4AC0D", color = "white", alpha = 0.7, bins = 15) +
  facet_wrap(~ gender_id, nrow = 2) +
  theme_minimal() +
  labs(title = "",
       y = "",
       x = "Age") +
  theme(axis.text = element_text(family = "sans", size = 12),
        axis.title.x = element_text(family = "sans", size = 18, margin = margin(20, 10, 10, 10)),
        strip.text = element_text(family = "sans", size = 15))

```

A little less than half of the educators were teachers (49%), followed by administrators (16%) and other classified staff (16%). The remaining of the sample (19%) was comprised of educational assistants, counselors, and other certified staff. A little more than half of the educators had been in their current career position, regardless of school site, for over 10 years. About 12% of the educators had been in their current career position for less than a year. 

All but four of the educators were born in the United States (U.S.). These four educators traced back their roots to Mexico or El Salvador and report coming for the first time to the U.S. when they were between 11 and 24 years of age. The entirety of the educators in the sample felt either *very comfortable* or *comfortable* speaking in English, but only about 20% felt the same way speaking in Spanish.

```{r echo=FALSE, fig.height= 5, fig.width=7, fig.cap="Educator's Races/Ethnicities and Roles", fig.align="center"}

tidy_d <- elt_w1_clean_6 %>%
  select(4, 5, 136, 139:147, 151, 152, 163, 164) %>%
  gather(race, count, 4:12) %>%
  filter(count == 1)

viz_2 <- tidy_d %>%
  mutate(race = as.factor(race),
         race = fct_recode(race,
                           "White" = "white",
                            "Other race/ethnicity" = "race_ethnicity_other",
                            "Indigenous Latin America" = "indigenous_americas",
                            "Latino" = "hispanic_latino_spanish",
                            "African American" = "black_african_american",
                            "Native American" = "american_indian_alaska_native"),
         years_in_position = as.factor(years_in_position),
         participant_role = as.factor(participant_role),
         participant_role = fct_recode(participant_role,
                           "Administrator" = "1",
                            "Teacher" = "2",
                            "Counselor" = "3",
                            "Educational Assistant" = "4",
                            "Classified staff" = "5",
                            "Certified staff" = "6"))

  
ggplot(viz_2, aes(fct_relevel(race, "White", "Latino"))) +
  geom_bar(aes(fill = participant_role), position = position_dodge2(preserve = "single")) +
  scale_y_continuous() +
  scale_fill_viridis_d("Educator's roles", option = "plasma") +
  theme_minimal() +
  labs(title = "",
       y = "",
       x = "") +
  theme(axis.text.x = element_text(family = "sans", size = 12, angle = 45, hjust = 1),
        legend.text = element_text(family = "sans", size = 10)) 
```

As can be seen in figure 6, educators in this sample were overwhelmingly White (81%). Other educators identified as Latino (12%), Native American (7%), Indigenous from Latin America (2%), and African American (2%). Roughly 5% identified as other race/ethnicity ^[In this study, participants were allowed to select as many races or ethnicities they felt identified with, thus, percent of total adds up to more than 100%].


# Study constructs' scales {-}

In this section, I describe the scale creation process of some of the study constructs the intervention was designed to influence. The steps involved in this process were: reverse coding of items, scale reliability check, and scale creation.

## Reverse coding of items {-}

The first step in the scale creation process was to identify which items needed to be reverse coded. Unfortunately, there was no indication in the original survey of which items needed to be reverse coded, so I had to use my subjective judgement to identify them. 

This was a time consuming step because it entailed an item by item and then a scale by scale review. Luckily, once the items were identified, the reverse coding process was really fast because the [likert_reverse](https://rdrr.io/github/martinctc/surveytoolbox/man/likert_reverse.html) function did the "heavy lifting"! Note that I use this function within a mutate function call.

In the code below I created a new dataframe called `elt_w1_clean_6_rev_code` where I included the new reverse coded variables. 

```{r}

elt_w1_clean_6_rev_code <- elt_w1_clean_6 %>%
  mutate(q25 = likert_reverse(q25, top = 4, bottom = 1),
         q73 = likert_reverse(q73, top = 4, bottom = 1),
         q110 = likert_reverse(q110, top = 6, bottom = 1),
         q113 = likert_reverse(q113, top = 6, bottom = 1),
         q114 = likert_reverse(q114, top = 6, bottom = 1),
         q115 = likert_reverse(q115, top = 6, bottom = 1),
         q116 = likert_reverse(q116, top = 6, bottom = 1),
         q123 = likert_reverse(q123, top = 4, bottom = 1),
         q124 = likert_reverse(q124, top = 4, bottom = 1))
```

Here I reverse coded all of the items in the survey that needed to be reverse coded, however, I did not use the first and last two variables, `q25`, `q123`, and `q124` in the scale creation process because they belong to scale constructs other than school climate, family-school relationships, and teacher self efficacy. The following are the items I reverse coded and used in the scale creation process described below: 

> 
Item q73: *Regardless of Latino students’ abilities, it seems most Latino students are bound for a vocational career rather than for community college or university studies.*  
Item q110: *When I really try, I can get through to most difficult students.*  
Item q113: *If a student did not remember information I gave in a previous lesson, I would know how to increase his/her retention in the next lesson.*  
Item 114: *If a student in my class becomes disruptive and noisy, I feel assured that I know some techniques to redirect him/her quickly.*  
Item q115: *If one of my students couldn't do a class assignment, I would be able to accurately assess whether the assignment was at the appropriate level of difficulty.*  
Item q116: *I can get through to even the most difficult or unmotivated students.*  


## Reliability check {-}

Once I completed the reverse coding step, I checked the reliability of the scales of the study constructs mentioned before. I used Chronbach's alpha as a measure of the internal consistency of the scale and I followed these guidelines to indicate the level of the scale's reliability:

> 
.00 to .69 = Poor reliability  
.70 to .79 = Fair reliability  
.80 to .89 = Good reliability  
.90 to .99 = Excellent/Strong reliability  

In the following code chunks, first, I created a dataframe with only the specific items of interest. Then, I checked the internal consistency of this dataframe (i.e., the scale) using the [alpha](https://www.rdocumentation.org/packages/psych/versions/2.0.12/topics/alpha) function. I followed this same procedure for all of the scales. 

### School climate

In the original survey that educators completed, the prompt for items q1 to q24 asked about the school's general climate. Despite my efforts to find the original measure that was used as a reference to create these items, I was not succesful in finding it. 

For what I gathered of the survey development process, different items from different measures were used, however, the survey developer(s) did not leave a precise record of what items belonged to what measure.

It is likely that several items from this scale were adapted from items in Part A: *General Climate Factors* of the public domain Charles F. Kettering Instrument (Fox et al., 1973) and *the Omnibus T-Scale* (Hoy & Tschannen-Moran, 2007). It is important to note that these two measures have a number of subscales within them, thus, it is possible that the scale named here as `climate_gen` also has subscales. This scale is a good candidate for further Exploratory and Confirmatory Factor Analyses (EFA and CFA).

```{r eval=FALSE}

climate_gen <- elt_w1_clean_6_rev_code %>%
  select(q1:q24) %>% 
  data.frame() 

alpha(climate_gen) # alpha = .92 -->  excellent consistency
```

Following the guidelines stated above, the general school climate scale has an excellent internal consistency (.92). It is important to say, however, that because Cronbach’s alpha increases as the number of items increases, this high score may be due to the high number of items included in the scale (24 items). 

Items within this scale had response options that ranged from 1 - 4 (i.e., strongly disagree - strongly agree). Higher scores in this scale indicate better general school climate. 

### Diversity engagement

For this scale, I was not able to locate any measures of reference.

```{r eval=FALSE}

diver_engage <- elt_w1_clean_6_rev_code %>%
  select(q36:q49) %>% 
  data.frame()

alpha(diver_engage) # alpha = .92 -->  excellent consistency
```

The diversity engagement scale has 14 items and it has an excellent internal consistency (.92), however, items within the scale seemingly allude to very different topics, from professional development to connecting student with resources. This scale may also be a good candidate for EFA and CFA. 

Items within this scale had response options that ranged from 1 - 4 (i.e., strongly disagree - strongly agree). Higher scores in this scale indicate more engagement of diverse students/families at the school. 

### Equity self-efficacy

For this scale, I was not able to locate any measures of reference.

```{r eval=FALSE}

equity_self_eff <- elt_w1_clean_6_rev_code %>%
  select(q50:q55) %>% 
  data.frame()

alpha(equity_self_eff) # alpha = .89 -->  good consistency
```

The equity self-efficacy scale has 5 items and it has a good reliability (.89).

Items within this scale had response options that ranged from 1 - 4 (i.e., strongly disagree - strongly agree). Higher scores in this scale indicate that educators were more self efficacious when promoting equity at their school. 

### Relationship with Latino families

For these scales, I was not able to locate any measures of reference.

In the original survey that educators completed, there were two sections about relationships with Latino families. The first section prompted educators to think about the school-Latino Families relationships and the second section prompted educators to think about their own relationship with Latino families. 

#### School-Latino families relationship

In the teacher-Latino families section of the survey, I noticed that 3 items referred more to the school-Latino families relationship than to the teacher-Latino families relationship:

>
Item q70: *The school makes Latino parents aware of opportunities for leadership roles in the school such as the Parent-Teacher Association or other decision-making opportunities.*  
Item q71: *Community resources and information are readily available and in Spanish at this school.*  
Item q72: *This school reaches out to community organizations that focus on Latino families.*  

I checked the reliability values of this scale with and without these 3 items.

```{r eval=FALSE}

scho_lat_fam_rel <- elt_w1_clean_6_rev_code %>%
  select(q56:q64) %>% 
  data.frame() # alpha = --> .84 good consistency

alpha(scho_lat_fam_rel)

scho_lat_fam_rel_2 <- elt_w1_clean_6_rev_code %>%
  select(q56:q64, q70:q72) %>% 
  data.frame() 

alpha(scho_lat_fam_rel_2) # alpha = .88 --> good consistency
```

As shown in the code above, the scale with the 3 additional items has a slightly better internal consistency (.88) so I decided to keep those items in the scale.

The school-Latino families relationship scale has 12 items and it has a good reliability (.88).

Items within this scale had response options that ranged from 1 - 4 (i.e., strongly disagree - strongly agree). Higher scores in this scale indicate better school-Latino families relationships. 

#### Teacher-Latino families relationship

I conducted a similar process for the teacher-Latino families relationship scale. First I checked the internal consistency of the scale as it was presented to the educators (i.e., as a section) and then I checked the internal consistency of the scale without the items I included in the school-Latino families relationship.

In the code below it can be seen that the internal consistency of the `teach_lat_fam_rel` scale was poor (.69). Applying the [alpha](https://www.rdocumentation.org/packages/psych/versions/2.0.12/topics/alpha) function threw a message that indicated that items q73 and q75 were negatively correlated with the total scale and a possible solution was reverse coding them. 

```{r eval=FALSE}

teach_lat_fam_rel <- elt_w1_clean_6_rev_code %>%
  select(q65:q76) %>%
  data.frame() # (q73 was reverse coded)

alpha(teach_lat_fam_rel) # alpha = .69 --> poor consistency, prblematic items: q73 and q75
```

Item q73 was actually one of the items I reverse coded at the beginning of this section and I don't think it should be reversed back. Item q75 did not appear to need to be reverse coded: 

> 
Item q73: *Regardless of Latino students’ abilities, it seems most Latino students are bound for a vocational career rather than for community college or university studies.*  
Item q75: *In general, I see Latino parents supporting their children's education, such as going to the library or helping with homework.* 

In my view, q73 does not hang well with the rest of the items in the scale as it does not probe the teacher-family relationship. What it does probe is a bias about Latino students. 

For its part, item q75, does not appear to probe the teacher-family relationship either, but the teacher's perception of Latino parents' involvement in their children education at home. Even though item q76 was not flagged, it appears to probe something similar to q75, the teacher's perception of Latino parents' involvement in their children education at school. I checked how the scale fared taking out these 3 items. 

```{r eval=FALSE}

teach_lat_fam_rel_2 <- elt_w1_clean_6_rev_code %>%
  select(q65:q72, q74) %>% # keeping q70:q72, leaving out q73, q75, q76
  data.frame()

alpha(teach_lat_fam_rel_2) # alpha = .76 --> fair consistency
```

As can be seen, taking out these items improved the Chronbach's alpha values of the `teach_lat_fam_rel_2` scale (.76, fair consistency). Despite the improvement, I still wanted to check the internal consistency of the scale if I took out the items I consider better reflect the relationship with school.

```{r eval=FALSE}

teach_lat_fam_rel_3 <- elt_w1_clean_6_rev_code %>%
  select(q65:q69, q74) %>% # leaving out q70:q73, q75, q76
  data.frame()

alpha(teach_lat_fam_rel_3) # alpha = .84--> good consistency
```

In the `teach_lat_fam_rel_3` scale I left out items q70 to q72 and the Chronbach's alpha score improved substantially, thus, I settled for this scale to measure the teacher-Latino family relationship. 

The final teacher-latino families relationship scale has 6 items and it has a good reliability (.84).

Items within this scale had response options that ranged from 1 - 4 (i.e., strongly disagree - strongly agree). Higher scores in this scale indicate better teacher-Latino families relationships.   

### Teacher self efficacy

Items to measure teacher's self efficacy were adapted from the Hoy & Woolfolk (1993) version of the *Teacher Efficacy Scale* (TES; Gibson & Dembo, 1984). Hoy & Woolfolk (1993) proposed and tested two independent dimensions in the TES, general teaching self efficacy and personal teaching self efficacy. Here I checked the reliability of the complete scale and of the two independent dimensions. 

Its important to note that the response options in this scale differed substantially from the previous scales. Response options ranged from 1 - 6 (i.e., strongly agree - strongly disagree). Also, Items q110, q113, q114, q115, q116 were reverse coded so that higher scores, either in the whole scale or the independent dimensions, indicate better teacher self efficacy. 

```{r eval=FALSE}

teach_self_eff_all <- elt_w1_clean_6_rev_code %>% 
  select(q108:q117) %>% 
  data.frame() # q110, q113, q114, q115, q116 were reverse coded)

alpha(teach_self_eff_all) # alpha = .74 --> fair consistency
```

Chronbach's alpha results indicate that the complete scale has a fair reliability (.74). In terms of the dimensions, the general teaching self efficacy scale has a fair reliability (.73) and the personal teaching self efficacy has a good reliability (.88).

```{r eval=FALSE}

teach_eff_gen <- elt_w1_clean_6_rev_code %>% # general teaching efficacy dimension in original measure
  select(q108, q109, q111, q112, q117) %>% 
  data.frame()

alpha(teach_eff_gen) # alpha = .73 --> fair consistency

teach_eff_per <- elt_w1_clean_6_rev_code %>% # personal teaching efficacy dimension in original measure
  select(q110, q113, q114, q115, q116) %>% 
  data.frame() # all items were reverse coded)

alpha(teach_eff_per) # alpha = .88 --> good consistency
```

I decided to use the independent dimensions instead of the whole scale as they provide more nuanced information. 

The general teaching self efficacy scale has 5 items. Higher scores indicate better general teaching self efficacy. 

The personal teaching self efficacy scale has also 5 items. Higher scores indicate better personal teaching self efficacy.


## Scale creation {-}

The last step in this process was to create the scales with the best reliability values that were checked before. 

In the code below I created a new data frame `elt_w1_scales` that only has id variables and the newly created scales. I created the new scales within a [mutate](https://dplyr.tidyverse.org/reference/mutate.html) call and I used the [rowwise](https://dplyr.tidyverse.org/articles/rowwise.html) function to conduct row-wise operations. 

```{r}
elt_w1_scales <- elt_w1_clean_6_rev_code %>%
  rowwise() %>% 
  mutate(climate_general = mean(c(q1, q2, q3, q4, q5, q6, q7, q8, q9, q10, q11, q12, q13, q14, q15, q16, q17, q18, q19, q20, q21, q22, q23, q24), na.rm = TRUE),
         school_engage_diversity = mean(c(q36, q37, q38, q39, q40, q41, q42, q43, q44, q45, q46, q47, q48, q49), na.rm = TRUE),
         equity_self_efficacy = mean(c(q50, q51, q52, q53, q54, q55), na.rm = TRUE),
         school_lat_fam_rel = mean(c(q56, q57, q58, q59, q60, q61, q62, q63, q64, q70, q71, q72), na.rm = TRUE),
         teacher_lat_fam_rel = mean(c(q65, q66, q67, q68, q69, q74), na.rm = TRUE),
         gen_teaching_efficacy = mean(c(q108, q109, q111, q112, q117), na.rm = TRUE),
         per_teaching_efficacy = mean(c(q110, q113, q114, q115, q116), na.rm = TRUE)) %>%
  select(1, 3, 4, 169:175) # selecting only id variables and the new scales
```

Finally, the code below shows how I merged, (or *joined* in R lingo), the scales' dataframe `elt_w1_scales` and the complete cleaned dataset called `elt_w1_clean_5`. I am using `elt_w1_clean_5` because in this version of the dataset the `recode_missing_d()` function that strips out variable and value labels was not yet applied. This means too that the final cleaned dataset will not have reverse coded items. I am still working on figuring out if there is a way I can include them. 

```{r}

final_elt_w1 <- left_join(elt_w1_clean_5, elt_w1_scales, by = c("school_id", "response_id", "id")) 
```

The final wave 1 educator dataset called now `final_elt_w1` is the dataset that will be exported to an SPSS file and the codebook that appears in the Appendix section in this report includes all of its variables and response options, with variable and value labels. 

\pagebreak

## Average scores {-}

In this final section, I include a table with the educator's baseline average scores in regards to the scales that I created above.

```{r include=FALSE}

# descriptive stats function

# function that takes a df and returns a df with only the numeric columns
only_numeric <- function(df) {
  select_numeric <- dplyr::select_if(df, is.numeric)
  return(select_numeric)
}

# a list with the functions we want
smry <- list(M = function(x) round(mean(x, na.rm = TRUE), 2), 
             SD = function(x) round(sd(x, na.rm = TRUE), 2),
             Min = function(x) round(min(x, na.rm = TRUE), 2),
             Max = function(x) round(max(x, na.rm = TRUE), 2))

# wrapping solution in a function
descriptives <- function(df) {
  select_numeric <- only_numeric(df)
  mean_sd <- map_df(select_numeric, function(col) map_df(smry, ~.x(col)),
       .id = "column")
  return(mean_sd)
}
```


```{r echo=FALSE}
# using the function descriptives I created above

elt_w1_scales_2 <- elt_w1_scales %>%
  select(-1:-3) %>%
  rename(c("General school climate" = "climate_general"), 
         c("Diversity engagement" = "school_engage_diversity"),
         c("Equity self efficacy" = "equity_self_efficacy"),
         c("School-Latino families rel." = "school_lat_fam_rel"),
         c("Teacher-Latino families rel." = "teacher_lat_fam_rel"),
         c("General teaching efficacy" = "gen_teaching_efficacy"),
         c("Personal teaching efficacy" = "per_teaching_efficacy"))

descrip_table <- descriptives(elt_w1_scales_2) %>%
  rename(c("Scale" = "column"))
```

```{r echo=FALSE}

descrip_table %>%
  kbl() %>%
  kable_material(c("striped", "hover", font_size = 7)) 
```


# Recommendations  {-}

* Survey development software
  + Assign a unique identifier for each participant and a different unique identifier per family. Additional unique identifiers that survey development software include can be handy when dealing with duplicated ids.
  + When developing the id protocol for schools, make sure that both values and value labels coincide in the survey development software that is being used. Using the value labels button in SPSS or the view_df() function in R can help identify discrepancies.
  + When a survey is being developed, have several team members go over the response options in the survey development software to check for potential coding errors.
  + If the survey development software provides the option to indicate if responses are either single or multiple responses, make sure that the option coincides with the type of question. That is, if response options are mutually exclusive select the single answer option; if response options are *NOT* mutually exclusive, select the multiple answer option.
  + Avoid including survey instructions in the variable labels so that they are not too long. 
 
* Questionnaire development
  + When a questionnaire is being developed for the first time, leave a precise record of where items are taken from. It is a good practice to include the whole citation of the measure that served as a reference and preferably download and save the measure because internet links can get broken or reused over time.
  + Try to use scales as they were designed. Cherry-picking items from different measures can lead to difficulties at identifying what is the construct being measured.
  + Always indicate what items need to be reverse coded. A simple asterisk sign by the item may save lots of time at the backend.
  + With items originally created for a project, it is a good idea to at least conduct an EFA to identify if the scale may have subscales. Chronbach's alpha values on scales that have a high number of items can be deceiving as the number of items increases Chronbach's alpha values increase too.  


# Appendix  {-}

```{r echo=FALSE}

view_df(final_elt_w1)
```

# References {-}

  Fox, R. S., Boles, H. E., Brainard, E., Fletcher, E., Huge, J. S., Martin, C. L., Maynard, W., Monasmith, J., Olivero, J., Schmuck, R., Shaheen, T. A., Stegeman, W. H. (1973). *School climate improvement: a challenge to the school administrator*. Bloomington, IN: Phi Delta Kappa Educational Foundation.

  Gibson, S., & Dembo, M. H. (1984). Teacher efficacy: A construct validation. *Journal of Educational Psychology*, 76(4), 569.

  Hoy, W. K., & Tschannen-Moran, M. (2007). *The conceptualization and measurement of faculty trust in schools: The Omnibus T-Scale*. In Hoy, W. K., & DiPaola, M. (Eds.). Essential ideas for the reform of American schools (pp. 87-114). Charlotte, NC: IAP.

  Hoy, W. K., & Woolfolk, A. E. (1993). Teachers' sense of efficacy and the organizational health of schools. *The Elementary School Journal*, 93(4), 355-372.
